{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDxGYC_yqTq"
      },
      "source": [
        "# SBU CSE 352 - HW 4 - Machine Learning From Scratch\n",
        "\n",
        "\n",
        "All student names in group: Blake Lewinski, Liana Torpey\n",
        "\n",
        "I understand that my submission needs to be my own group's work: B.C.L., L.G.T.\n",
        "\n",
        "##### I understand that ChatGPT / Copilot / other AI tools are not allowed: B.C.L., L.G.T.\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Total Points: 100\n",
        "\n",
        "1. Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Only use standard packages (numpy and built-in packages like random). Submit the completely rendered notebook as a HTML file.\n",
        "\n",
        "  **Important:** Do not use scikit-learn or other packages with ML built in. The point of this is to be a learning exercise. Using linear algebra from numpy is okay (things like matrix operations or pseudoinverse, for example, but not lstsq).\n",
        "\n",
        "2. Your notebook needs to be formatted professionally.\n",
        "    - Add additional markdown blocks for your description, comments in the code, add tables and use matplotlib to produce charts where appropriate\n",
        "    - Do not show debugging output or include an excessive amount of output.\n",
        "    - Check that your PDF file is readable. For example, long lines are cut off in the PDF file. You don't have control over page breaks, so do not worry about these.\n",
        "3. Document your code. Add a short discussion of how your implementation works and your design choices.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement several machine learning algorithms and evaluate their accuracy. This will be done for a downscaled version of the MNIST digit recognition dataset.\n",
        "\n",
        "**Like in real life, some of the tasks you will be asked to do may not be possible, at least directly. In these cases, your job is to figure out why it won't work and either propose a fix (best), or provide a clear explanation why it won't work.**\n",
        "\n",
        "For example, if the problem says to do k-nearest neighbors with a dataset of a billion points, this could require too much time to do each classification so it's infeasible to evaluate its test accuracy. In this case, you could suggest randomly downsample the data to a more manageable size, which will speed things up by may lose some accuracy. In your answer, then, you should describe the problem and how you solved it and the trade-offs.\n",
        "\n",
        "# Data\n",
        "First the code below ensures you have access to the training data (a subset of the MNIST images), consisting of 100 handwritten images of each digit."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First download the repo and change the directory to be the one where the\n",
        "# dependencies are. You should only need to do this once per session. If you\n",
        "# want to reset, do Runtime -> Disconnect and Delete Runtime\n",
        "# You can always do !pwd to see the current working directory and !ls to\n",
        "# list current files.\n",
        "!git clone https://github.com/stanleybak/CS7320-AI.git\n",
        "%cd CS7320-AI/ML\n",
        "!ls"
      ],
      "metadata": {
        "id": "dTw87RlBzTOi",
        "outputId": "b0a380ff-b385-40a1-f8f4-6cb0b8fd50ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS7320-AI'...\n",
            "remote: Enumerating objects: 2738, done.\u001b[K\n",
            "remote: Counting objects: 100% (855/855), done.\u001b[K\n",
            "remote: Compressing objects: 100% (372/372), done.\u001b[K\n",
            "remote: Total 2738 (delta 522), reused 796 (delta 478), pack-reused 1883\u001b[K\n",
            "Receiving objects: 100% (2738/2738), 285.30 MiB | 14.23 MiB/s, done.\n",
            "Resolving deltas: 100% (1690/1690), done.\n",
            "Updating files: 100% (135/135), done.\n",
            "/content/CS7320-AI/ML\n",
            "line_fitting.ipynb\tML_example.ipynb\tML_for_tictactoe_self_play.ipynb  README.md\n",
            "mini-mnist-1000.pickle\tML_for_tictactoe.ipynb\tMNIST.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ny3IAxVAyqTs",
        "outputId": "1f57d494-fb2b-4faf-d7c4-02b17ea9e475"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFQUlEQVR4nO3dTShsYRzH8SFSFjSzYyFKNixkKWHhJRNZKYWMZCmWytZa2VDKxl7NENKUlOxnYWs1FkPZUN7jrm/P/9xmjDP3/M75fpb/njvOvX3vqfNinqrv7+/vGCCm+n8fAPAThAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJNcUurKqq8vM4gFgsFosV+yCXMy4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kFf0VTFHT0dFhzmtra51Zf3+/uXZ7e9uZfX19lXdgJcpkMs5senraXPv+/u734fwazriQRLiQRLiQRLiQRLiQVFXslqhh+GLnzs5Oc55KpZzZ1NSUuba62v2/3tzcbK61/s2CsAPt/v6+OV9dXXVmj4+PPh/N3/hiZ4Qa4UIS4UIS4UJSpC7ODg8PzXkymfTl5wX14szLwMCAM7u6uqroMXBxhlAjXEgiXEgiXEgiXEiK1Ivk2WzWnJdyV+H+/t6Z7e3tmWutx8OlvEje29trzq2r/6jhjAtJhAtJhAtJhAtJkXrkW1NjX4s2NTUV/RkfHx/OrFAo/PiY/qWhocGcX19fOzOvd4It6XTanM/MzDizt7e3oj/3N/DIF6FGuJBEuJBEuJBEuJAUqUe+n5+f5jyfz1f4SIozOjpqzuPxeFmfe3t7a84rfQehHJxxIYlwIYlwIYlwISlSj3yDzPqy5aWlJXNtue/jJhIJc17pr1uy8MgXoUa4kES4kES4kES4kBSpR76VZr2Yvba2Zq5tb293ZtbWVKXK5XLOzHoZXg1nXEgiXEgiXEgiXEiK1MVZa2urOZ+bm3NmQ0NDZf+8vr4+Z/YbX+xsPZr1uug7OTlxZi8vL2Ufw//GGReSCBeSCBeSCBeSCBeSQvsieVdXlzPz2i6qpaXFl2Pwa7uo4+NjZzY5OVn25wYBL5Ij1AgXkggXkggXkiL1yNfrAtOvC89yd93xMj4+7szGxsbMtaenp2X/vCDijAtJhAtJhAtJhAtJhAtJob2rYG2pNDg4aK6dnZ11ZmdnZ+ba19fXso7Ly+LiojNbXl725WeFAWdcSCJcSCJcSCJcSArt+7hqGhsbndnDw0PRf35iYsKcqz3y5X1chBrhQhLhQhLhQhLhQlJoH/mq8dq3FzbOuJBEuJBEuJBEuJAkdXFm7UIzMjJirj0/P3dmQfhC44WFBXO+tbVV4SPRxhkXkggXkggXkggXkggXkgJ5V8HaZikWi8XW19ed2fDwsLm2ra3NmeXz+fIOzEMikTDnyWTSmW1ubppr6+vri/551t0Rv377OKg440IS4UIS4UIS4UJSIH/LN5fLmXNrJx0vOzs7zuzp6emnh/RPXheIPT09zqyUXXcuLi7MufV3Ozg4KPpzg4zf8kWoES4kES4kES4kES4khfauQhBY/2Z3d3fm2qOjI2e2srJirg3z413uKiDUCBeSCBeSCBeSAnlx1t3dbc6tXWjm5+d9Ppq/3dzcOLPn52dz7eXlpTPb3d0111q7BEURF2cINcKFJMKFJMKFJMKFpEDeVfBSV1fnzFKplLl2Y2PDmcXjcXNtOp12Ztls1lybyWScWaFQMNeidNxVQKgRLiQRLiQRLiRJXZwh/Lg4Q6gRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQVvZdvKdscAX7jjAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJfwBCXTEvdHk8ngAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# if the below fails to open, then the data file is not in the current working\n",
        "# directory (see above code block)\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "im3 = data['images'][300] # 100 images of each digit\n",
        "plt.figure(figsize=(2, 2))  # Adjust size as needed\n",
        "plt.imshow(im3, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzts6NT5yqTt"
      },
      "source": [
        "# Downscaling Images\n",
        "\n",
        "MNIST images are originally 28x28. We will train our models not just on the original images, but also on downscaled images with the following sizes: 14x14, 7x7, 4x4, 2x2. The next code block shows one way to do downscaling. As you can tell from the output, we cannot expect our model's accuracy will be too high on lower resolution versions, although it's unclear how much better you can do than random chance, which should have a 10% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2wTIXhvGyqTt",
        "outputId": "0d68317a-a357-4c26-988b-89e95848efa0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAamUlEQVR4nO3de1BU5/nA8WcBEVCLQwJqUUEiSovXVNQyCaLR2BpLvdAYm4r3mPFSmzaxccLEEgfrNWOUmJgbKrUGTYI6NhZT8TKNVtSojUKN14gGNeAVFRT2/f2RcX+uZ2EPsC+7q9/PDH+cZ599z4s+4nl4z7vHopRSAgAAAAAu5uPuCQAAAAB4MNFsAAAAANCCZgMAAACAFjQbAAAAALSg2QAAAACgBc0GAAAAAC1oNgAAAABoQbMBAAAAQAuaDQAAAABaeFWzERkZKWPGjHH3NPCQov7gLahVuBs1CHei/jyLRzQbX3/9tSQnJ0tERIQEBARIeHi4DBgwQJYuXerWeZWWlsqCBQskISFBQkNDpXnz5tK7d2/Jzs52mH/s2DF57rnnpHXr1hIUFCQxMTHyxhtvyM2bN73q3A8bT60/EZHs7Gz53e9+J9HR0WKxWCQxMdHU+9LT08VisUinTp288txwzFNrdfv27WKxWKr9Sk9Pr/WYkZGR1Y4XHR2t4buAGZ5ag/c7ceKEBAQEiMVikX379rlkzAEDBojFYpGpU6e6ZDzUnqfWX22v2czYunWrjBs3Tjp06CBBQUESFRUlEyZMkOLiYhfOvGFYlFLKnRPYtWuX9O3bV9q2bSujR4+Wli1bSlFRkfznP/+REydOyPHjx225FRUV4uPjI40aNWqQuW3atEmGDRsmgwYNkr59+4qfn598+umnsm3bNnn99dclLS3NlltUVCRdunSR4OBgefHFFyUkJER2794tK1askKSkJNmwYYPXnPth4sn1JyKSmJgo+/fvl7i4ODl48KB06dJFtm/fXuN7zp49Kx07dhSLxSKRkZFy+PBhrzs3jDy5Vi9cuCBffPGFIZ6VlSVbtmyR/Px8iYuLq9WY69evl7KyMrvYt99+K6mpqTJ58mR5++236zVn1J4n1+D9kpKSJC8vT27cuCF79+6VHj161Gu8zz77TFJSUuTGjRsyZcoUycjIcNFMYZYn119trtnM6tGjh1y6dEl+85vfSHR0tJw8eVIyMjIkKChIDh48KC1bttTwnWii3GzQoEEqNDRUXb582fDahQsXGn5C9zh58qQ6ffq0Xcxqtap+/fqpxo0bq7KyMls8PT1diYg6fPiwXX5KSooSEXXp0iWvOffDxJPrTymlzpw5o6qqqpRSSsXGxqo+ffo4fc+IESNUv379VJ8+fVRsbKxXnhtGnl6rjrRv315FR0e7bLzZs2crEVFffvmly8aEed5Sg//85z+Vv7+/Sk1NVSKi9u7dW6/xbt26pSIjI9Ubb7yhRERNmTLFRTNFbXhy/dXmms2sHTt22P4PvjcmIuq1116r13wbmttvozpx4oTExsZK8+bNDa+FhYXZHd9/D15Ny/anT5+25f3vf/+T5ORkCQkJkYCAAOnRo4ds3LjR6dzatWsnERERdjGLxSJDhgyRiooKOXnypC1+7do1ERFp0aKFXX6rVq3Ex8dH/P39RUQkMzNTLBaLfPTRR3Z5c+bMEYvFIp9//rm2c8PIk+tPRKRNmzbi42P+n+nOnTvlk08+kcWLFzt83Wz96Tg36sfTa/V++fn5cvz4cXn++edtsYsXL0poaKgkJiaKumdR/fjx49KkSRMZMWJEjWP+/e9/l3bt2kl8fHyd5oT68YYavHPnjkyfPl2mT58ujz32mOH1utTg/PnzxWq1yssvv2x6HnA9T64/s9dst27dkpiYGImJiZFbt27Zci9duiStWrWS+Ph4qaqqEhGRhIQEw//BCQkJEhISIoWFhU7n5Enc3mxERETI/v3763S7RVZWluErIiJCAgMDpWnTpiIicuTIEendu7cUFhbKq6++KosWLZImTZrIkCFDJCcnp05zPn/+vIiIPProo7bY3fvZx48fLwcPHpSioiLJzs6Wd955R37/+99LkyZNRERk7NixMnjwYPnjH/8oRUVFIvLDPYhpaWkyfvx4GTRokLZzw8gb6686VVVVMm3aNJkwYYJ07tzZYU59668+50b9eFutrl69WkTErtkICwuTd955R3bs2GG7x9pqtcqYMWOkWbNmsmzZsmrHO3DggBQWFspvf/vbWs8FruENNbh48WK5fPmypKamOny9tjV45swZmTt3rsybN08CAwNr/X3Ddbyh/u53/zVbYGCgrFy5Uo4fPy6vvfaaLW/KlCly9epVWbFihfj6+lY7XllZmZSVldldA3oFdy+tbNmyRfn6+ipfX1/185//XM2YMUPl5uaq27dvG3IjIiLU6NGjqx1r/vz5SkTUqlWrbLGnnnpKde7cWZWXl9tiVqtVxcfH12l5v7S0VIWFhaknn3zS8Nrs2bNVYGCgEhHbl6OlruLiYhUSEqIGDBigKioqVPfu3VXbtm3V1atXtZ8b9ryp/pzdypSRkaGCg4PVxYsXlVKq2luZ6lJ/rjo36s6barWyslK1aNFC9ezZ0+HrI0eOVEFBQeqbb75RCxYsUCKi1q9fX+OYf/rTn5SIqIKCglrNBa7j6TVYXFysmjVrppYvX66UUiozM7Pa26jM1mBycrKKj4+3HQu3UbmNp9ff/Wq6Zps5c6by8fFRO3fuVOvWrVMiohYvXux0zLu3km7durXW83EntzcbSimVn5+vhg4dqoKCgmwXyqGhoWrDhg12eTUVT15envL19VXTpk2zxUpLS5XFYlGzZ89W33//vd1XWlqaEhF19uxZ0/OsqqpSv/jFL5S/v786ePCg4fWsrCw1cOBA9d5776lPP/1UjRs3TlksFrV06VJD7po1a5SIqJ49eyqLxaL+9a9/Ndi5Yc9b6q+mC/6SkhIVEhKiFi5caIvVdMFf2/pz5blRd95Sq7m5uUpE1FtvveXw9dLSUtWqVSvVpUsXFRAQoEaNGlXjeFVVVSo8PFx1797d9ByghyfXYEpKiuratavtPveamg0zNZiXl6csFovKz8+3xWg23MuT6+9ezq7ZKioqVOfOnVW7du1UaGio6tOnj7JarTWOuWPHDuXn56eeffZZ0/PwFB7RbNxVUVGh8vPz1cyZM1VAQIBq1KiROnLkiO316oqnqKhIhYaGqoSEBHXnzh1bfM+ePXa/6Xf09dVXX5me3+TJkw2d8F1r1qxRgYGBqqioyC4+ZswYFRQUpEpKSgzveeaZZ5SIqBdeeKHBzw0jT6+/mi74X3zxRdW+fXtVUVFhizm74K9N/bn63KgfT6/VlJQU5evrq86fP19tzt3f5rVo0cLhhs975eXlKRGxa2jhXp5Wg7t371YWi0Xl5eXZYjU1G0rVXIN37txRnTp1UikpKXZxmg3P4Gn1d7+artnu2rt3rxIRFRAQoE6ePFnjeIWFhSokJER169ZNXbt2zfQ8PIWf6futGoC/v7/ExcVJXFycdOjQQcaOHSvr1q2TWbNmVfue27dvS3JysjRu3FjWrl0rfn7//y1ZrVYREXn55Zdl4MCBDt/fvn17U3NLS0uTZcuWydy5c2XUqFGG15ctWybdu3eX1q1b28WTkpJkxYoVcuDAAenfv78tXlpaavvs74KCArFardVuxnX1ueGYJ9dfTY4dOybvvfeeLF68WL777jtbvLy8XO7cuSOnT5+WH/3oRxISEmJ7rTb15+pzo/48uVZv3bolOTk50r9/f8OHVtwrNzdXREQuX74sZ8+edbjp867Vq1eLj4+PjBw50tQcoJ+n1eCMGTPkySeflHbt2tk2/JaUlIiISHFxsZw5c0batm1r956aanDVqlVy9OhRWb58ud0GYhGR69evy+nTpyUsLEyCgoKqnRP08bT6u5eza7a77tZfeXm5HDt2TNq1a+cwr6ioSJ5++mkJDg6Wzz//XJo1a2ZqHh7F3d1Odb7++mslImrSpEm2mKNOddKkSapx48Zqz549hjEuXLigRETNnDmzXnPJyMhQIqL+8Ic/VJvToUMH1atXL0M8OztbiYjavHmzXXzEiBEqKChI/fWvf1UiohYtWtRg54ZznlR/d1W3urBt2zanv5GZPn263XvM1p+Oc8O1PK1WP/74Y6e/0du8ebMSETVjxgwVHh6uHn/8cbvfMt6rvLxcNW/eXPXr16/ec4MenlCDERERNf4cCg4Otst3VoOzZs1y+rMtJyenTnOFa3lC/d1l5ppNKaUOHTqk/P391dixY1X37t1VmzZt1JUrVwx5JSUlKiYmRoWFhalvvvmmXnNzJ7c3G3l5eQ7vU5s3b54SEfXmm2/aYvcXz0cffaRERH3wwQfVjp+YmKhCQkLUd999Z3jt7mbWmnz88cfKx8dHPf/88zXeTzd48GDl7++vjh49ahcfMmSI8vHxUefOnbPF7i7dLlmyRCml1HPPPacCAwMN79Vxbtjz9Pq7V3UX/N9//73KyckxfMXGxqq2bduqnJwc9d///teWb7b+dJwbdecttZqUlKSCgoLU9evXHb5++fJlFR4ernr27KkqKyttF31paWkO8z/77DMlIurDDz80PQfo4ck1mJuba/g5NG3aNNvtd5s2bbLlmqnBwsJChz/bREQNGjRI5eTkOJwn9PHk+lPK/DXb7du3Vffu3VVkZKS6du2aXeNxr7KyMtWzZ0/VrFkztW/fPqfn92Ruf4J4p06d5ObNmzJ06FCJiYmR27dvy65duyQ7O1vatGkjBw4csC1tRkZGSmJioqxYsUJKSkqkTZs2EhUVJTNnzjSMO3ToUGnSpIkUFBTIE088IT4+PjJx4kSJioqSCxcuyO7du+Xs2bNy6NChaueWn58vTz75pAQHB8u8efMMT6KMj4+XqKgoEfnhGQP9+vWTRx55RKZOnSqPPPKIbNq0STZv3iwTJkyQ999/X0R++Izv2NhY6dy5s2zdulUsFouUlpZKbGysREVFyb///W/x8fHRcm4YeXL9ifzwd7tz504REVm6dKkEBQXJ+PHjReSHz9tOSEio9r2JiYlSUlJi9zGBZutPx7lRP55eqyI/fFZ8y5YtZfjw4bJmzRqHOaNHj5a1a9fKgQMHJCYmRkREJk6cKCtXrpS9e/dK165d7fKTk5Nl06ZNcuHCBQkODq7lnxpcyRtq8F4rVqyQsWPHGp4gXtsavJfFYuEJ4m7iyfVXm2u2WbNmyezZs2Xr1q3St29fERFJT0+X1NRU+cc//mH7CPohQ4bIhg0bZNy4cba8u5o2bSpDhgyp6x9lw3N3t7N582Y1btw4FRMTo5o2bar8/f1V+/bt1bRp0wxPhLy3Uz116lSNy5unTp2yve/EiRMqJSVFtWzZUjVq1EiFh4erwYMHq08++aTGud3dXFbdV2Zmpl3+nj171C9/+UvbeTp06KDS09PtlmaHDRummjVrZnjS5IYNG5SIqHnz5mk7N4w8uf6Uqnkpf9asWTW+19EmbbP1p+PcqB9Pr1WllHr33XeViKiNGzc6fP1und1/2961a9dURESE6tq1q93HWF69elUFBASoYcOGmTo/9PKGGryXow3ita3B+4mwQdxdPLn+zF6z7d+/X/n5+dl9EpZSP3xceFxcnPrxj39s+7CCmm4NjIiIqM8fZYNz+8oGAAAAgAeT258gDgAAAODBRLMBAAAAQAuaDQAAAABa0GwAAAAA0IJmAwAAAIAWNBsAAAAAtPAzm2ixWHTOA16qoT45mfqDIw35yd3UIBx5mH4GespDFSMjI909BTl//ry7pyAiDTePxx57rEHO4w2sVqu7p+AxTp06ZSqPlQ0AAAAAWtBsAAAAANCCZgMAAACAFjQbAAAAALSg2QAAAACgBc0GAAAAAC1oNgAAAABoQbMBAAAAQAuaDQAAAABa0GwAAAAA0IJmAwAAAIAWNBsAAAAAtKDZAAAAAKAFzQYAAAAALWg2AAAAAGhBswEAAABAC5oNAAAAAFrQbAAAAADQws/dEwDwg0aNGpnK8/Mz98+2d+/eTnOuXr1qaqxevXo5zSkrKzM1VlZWlqm8B5mvr6/LxkpKSnLZWJ06dXLZWCKu/T5LSkpcNlZGRobLxgIA1IyVDQAAAABa0GwAAAAA0IJmAwAAAIAWNBsAAAAAtGCD+D0cbWYMDg6u83hTp061Ow4KCjLkdOzY0RCbMmWKIbZw4UK745EjRxpyysvLDbG5c+caYmlpacbJAgAAAC7GygYAAAAALWg2AAAAAGhBswEAAABAC5oNAAAAAFp4/Qbxtm3bGmL+/v6GWHx8vCH2xBNP2B03b97ckDN8+PC6T86Es2fPGmJLliwxxIYOHWp3fP36dUPOoUOHDLEdO3bUY3YPt6ZNmzrNmTVrlqmxHn/8cac5Zp/e/Oijj5rKM8NRrTli5oMS1q1bV9/pAACABwwrGwAAAAC0oNkAAAAAoAXNBgAAAAAtvGrPRrdu3QyxvLw8Q6w+D+LTyWq1GmKpqamGWFlZmSG2evVqu+Pi4mJDzuXLlw2xo0eP1maKAAA4NHnyZHdPQUREdu7c6e4pyMWLF909BcBrsLIBAAAAQAuaDQAAAABa0GwAAAAA0IJmAwAAAIAWXrVB/MyZM4ZYaWmpIaZ7g/iePXsMsStXrhhiffv2tTu+ffu2IScrK8tl84JrlZeXO83p0KGDqbFat27tNCcgIMDUWBMmTDCVN2jQIKc5L730kqmx4FqOPiyirqKjo102VmBgoMvGEhH529/+5rKxCgoKXDYWAKDhsLIBAAAAQAuaDQAAAABa0GwAAAAA0IJmAwAAAIAWXrVB/NKlS4bYK6+8YogNHjzYEDtw4IAhtmTJEqfnPHjwoCE2YMAAQ+zGjRuGWGxsrN3x9OnTnZ4PAAAAeFCwsgEAAABAC5oNAAAAAFrQbAAAAADQgmYDAAAAgBZetUHckfXr1xtieXl5htj169cNsa5du9odjx8/3pCzcOFCQ8zRZnBHjhw5Ynf8wgsvmHofPENlZaXTnGHDhpka6+mnn3aak5qaamqszMxMl+YBAADowsoGAAAAAC1oNgAAAABoQbMBAAAAQAuaDQAAAABaeP0GcUeuXbtmKu/q1atOcyZOnGiIZWdnG2JWq9XUOQEAAICHBSsbAAAAALSg2QAAAACgBc0GAAAAAC0eyD0bZv3lL3+xO/7Zz35myOnTp48h1r9/f0Nsy5YtLpsXvEdVVZWpvNzcXKc5kydPNjXWSy+9ZCpv7dq1TnPOnTtnaiy4llLKZWPNnz/fZWOZfbCkWRERES4bq6CgwGVjAQAaDisbAAAAALSg2QAAAACgBc0GAAAAAC1oNgAAAABo8VBvEL9x44bdsaMH+H311VeG2Pvvv2+Ibdu2zRDbt2+f3fHbb79tyHHlRlEAAADAk7CyAQAAAEALmg0AAAAAWtBsAAAAANCCZgMAAACAFg/1BvH7nThxwhAbM2aMIZaZmWmIjRo1ymmsSZMmhpxVq1YZYsXFxTVNE17IarU6zRk7dqypsVauXGkqb8qUKU5znn32WVNjOfqgBAAPl+XLl7t7CiLi+ifd18WXX37p7inATfz9/d09Ba/DygYAAAAALWg2AAAAAGhBswEAAABAC5oNAAAAAFqwQdyJnJwcQ+zYsWOG2JtvvmmIPfXUU3bHc+bMMeREREQYYunp6YbYuXPnapwnAAAA4GlY2QAAAACgBc0GAAAAAC1oNgAAAABoQbMBAAAAQAs2iNfB4cOHDTFHT2L+1a9+ZXfs6MnjkyZNMsSio6MNsQEDBtRmivBCJSUlpvJ+/etfm8p76623nOZs2bLF1Fjt27d3mnPlyhVTY8Fzma0Hsz788EOXjfXFF1+4bKzKykqXjQUAqBkrGwAAAAC0oNkAAAAAoAXNBgAAAAAt2LPhIo7uV8/KyrI7/uCDDww5fn7Gv4KEhARDLDEx0e54+/bttZofAAAA0NBY2QAAAACgBc0GAAAAAC1oNgAAAABoQbMBAAAAQAs2iNdBly5dDLHk5GRDLC4uzu7Y0WZwRwoKCgyxnTt3mpwdvJWjunLkz3/+s6m8gQMHOs25fv26qbF4YB8AAKgLVjYAAAAAaEGzAQAAAEALmg0AAAAAWtBsAAAAANCCDeL36NixoyE2depUQ2zYsGGGWMuWLet0zqqqKkOsuLjYELNarXUaHwAAAHAXVjYAAAAAaEGzAQAAAEALmg0AAAAAWtBsAAAAANDiodkg7mgD98iRI+2OHW0Gj4yMdNkc9u3bZ4ilp6cbYhs3bnTZOaFXWFiYqby0tDSnOSkpKabGunPnjqm8ZcuWOc1ZtGiRqbHgWr1793bZWNOnT3fZWJWVlS4bS0Rk+PDhLhvL1XMDADQMVjYAAAAAaEGzAQAAAEALmg0AAAAAWnj9no0WLVoYYj/96U8NsYyMDEMsJibGZfPYs2ePIbZgwQK74w0bNhhyeFgfAMAZs/vDdHrmmWfcPQUREXn99dfdPQUAtcDKBgAAAAAtaDYAAAAAaEGzAQAAAEALmg0AAAAAWnj0BvGQkBC74+XLlxtyunXrZohFRUW5bA67du0yxBw9CC03N9cQu3XrlsvmAdcx+0A1Mw9LS0pKMjWWv7+/05x3333X1FizZ882lXfx4kVTeQAAALqwsgEAAABAC5oNAAAAAFrQbAAAAADQgmYDAAAAgBZu2SDeq1cvQ+yVV14xxHr27Gl3HB4e7tJ53Lx50+54yZIlhpw5c+YYYjdu3HDpPAAAAIAHESsbAAAAALSg2QAAAACgBc0GAAAAAC1oNgAAAABo4ZYN4kOHDjUVM6OgoMAQ27RpkyFWWVlpiN3/JPArV67UaQ7wLj/5yU9M5RUWFjrNWb9+vamx9u/f7zTn+PHjpsYC7vXqq6+6bKxvv/3WZWMBACDCygYAAAAATWg2AAAAAGhBswEAAABAC5oNAAAAAFq4ZYO4ow2NrtzkCAAAAMD9WNkAAAAAoAXNBgAAAAAtaDYAAAAAaOGWPRuAO2VmZrp7CgAAAA8FVjYAAAAAaEGzAQAAAEALmg0AAAAAWtBsAAAAANCCZgMAAACAFjQbAAAAALSg2QAAAACgBc0GAAAAAC1oNgAAAABoYVFKKXdPAgAAAMCDh5UNAAAAAFrQbAAAAADQgmYDAAAAgBY0GwAAAAC0oNkAAAAAoAXNBgAAAAAtaDYAAAAAaEGzAQAAAEALmg0AAAAAWvwfBYE9LLSOA8UAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to downscale an image to different sizes\n",
        "def downscale_image(image, downscaled_size):\n",
        "    block_size = 28 // downscaled_size\n",
        "    downscaled = np.zeros((downscaled_size, downscaled_size))\n",
        "    for i in range(downscaled_size):\n",
        "        for j in range(downscaled_size):\n",
        "            # Calculate the average for each block\n",
        "            block = image[i*block_size:(i+1)*block_size,\n",
        "                          j*block_size:(j+1)*block_size]\n",
        "            downscaled[i, j] = np.mean(block)\n",
        "    return downscaled\n",
        "\n",
        "# Load the dataset (assuming this file is in your working directory)\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "images = data['images']  # a list of 1000 numpy image matrices\n",
        "labels = data['labels']  # a list of 1000 integer labels\n",
        "\n",
        "# Select 3 \"random\" indices from the dataset\n",
        "random_indices = [300, 500, 200]\n",
        "\n",
        "# Downscale the images to multiple sizes and display them\n",
        "sizes = [28, 14, 7, 4, 2]\n",
        "for index in random_indices:\n",
        "    fig, axs = plt.subplots(1, len(sizes), figsize=(10, 2))\n",
        "    for ax, size in zip(axs, sizes):\n",
        "        downscaled_image = downscale_image(images[index], size)\n",
        "        ax.imshow(downscaled_image, cmap='gray', vmin=0, vmax=255)\n",
        "        ax.set_title(f'Size {size}x{size}')\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YePL7s9NyqTw"
      },
      "source": [
        "---\n",
        "# Tasks\n",
        "\n",
        "Your data contains 100 images in each class. When training models, use 80% of training, 10% for validation and 10% for testing. Make sure the data is balanced in each class when splitting.\n",
        "\n",
        "---\n",
        "## Task 1: Linear Classifier [20 points]\n",
        "\n",
        "First, implement a linear classifier. The simplest way to do this is to adapt linear regression approaches that we learned about in class, where the output is a real number. For classification, we can let one category be an output of 1.0 and the other -1.0. Then, after the classifier is trained we can use the sign of the output to determine the predicted class.\n",
        "\n",
        "However, since in MNIST there are multiple classes (10 digits, not just 2), we need to adapt the approach further. We will try both of the following two popular strategies: One-vs-Rest (OvR) and One-vs-One (OvO).\n",
        "\n",
        "**One-vs-Rest (OvR)** is a strategy for using binary classification algorithms for multiclass problems. In this approach, a separate binary classifier is trained for each class, which predicts whether an instance belongs to that class or not, making it the 'one' against all other classes (the 'rest'). For a new input instance, compute the output of all classifiers. The predicted class is the one whose corresponding classifier gives the highest output value.\n",
        "\n",
        "**One-vs-One (OvO)** is another strategy where a binary classifier is trained for every pair of classes. If there are N classes, you will train N(N−1)/2 classifiers. For a new input, evaluate it using all N(N−1)/2​ classifiers. Count the number of times each class is predicted over all binary classifications. The class with the highest count is selected as the final prediction.\n",
        "\n",
        "### Report Results\n",
        "Report the test accuracy for OvR and OvO, for each of the input image sizes, 28x28, 14x14, 7x7, 4x4, 2x2. A table may be helpful. Also report any interesting observations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "random.seed(10) # to make results repeatable\n",
        "\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "def data_split(data, size):\n",
        "    \"\"\"\n",
        "    Split the data (by stratified splitting) into training, validation, and\n",
        "    test sets.\n",
        "\n",
        "    Parameters:\n",
        "        data: A data set of images, represented as a dictionary.\n",
        "        size: Size of images to be used for classification.\n",
        "\n",
        "    Returns:\n",
        "        training_set: 80% of original data set to be used for training.\n",
        "        validation_set: 10% of original data set to be used for selecting\n",
        "            hyperparameters.\n",
        "        test_set: Last 10% of original data set to be used for testing\n",
        "            models.\n",
        "    \"\"\"\n",
        "\n",
        "    # use a tuple of images and labels rather than the data dictionary for\n",
        "    # more efficient computations\n",
        "\n",
        "    simplified_data = [(data['images'][i], data['labels'][i]) for i in\n",
        "                       range(len(data['images']))]\n",
        "\n",
        "    # separate the data by label classes for efficient stratified splitting\n",
        "    class_zero = simplified_data[:100]\n",
        "    class_one = simplified_data[100:200]\n",
        "    class_two = simplified_data[200:300]\n",
        "    class_three = simplified_data[300:400]\n",
        "    class_four = simplified_data[400:500]\n",
        "    class_five = simplified_data[500:600]\n",
        "    class_six = simplified_data[600:700]\n",
        "    class_seven = simplified_data[700:800]\n",
        "    class_eight = simplified_data[800:900]\n",
        "    class_nine = simplified_data[900:]\n",
        "    classes = [class_zero, class_one, class_two, class_three,\n",
        "            class_four, class_five, class_six, class_seven,\n",
        "            class_eight, class_nine]\n",
        "\n",
        "    # shuffle the classes to increase randomness amongst split sets\n",
        "    for c in classes:\n",
        "        random.shuffle(c)\n",
        "\n",
        "    training_set = list()\n",
        "    validation_set = list()\n",
        "    test_set = list()\n",
        "\n",
        "    # add 80% of each class to training set, 10% to validation set, and the\n",
        "    # remaining 10% to the test set\n",
        "    for c in classes:\n",
        "        training_set = training_set + [(downscale_image(vector[0], size),\n",
        "                                        vector[1]) for vector in c[:80]]\n",
        "        validation_set = validation_set + [(downscale_image(vector[0],\n",
        "                                            size), vector[1]) for vector in\n",
        "                                            c[80:90]]\n",
        "        test_set = test_set + [(downscale_image(vector[0], size),\n",
        "                                vector[1]) for vector in c[90:]]\n",
        "\n",
        "    return training_set, validation_set, test_set\n",
        "\n",
        "def get_weight_vectors(train_matrix, labels):\n",
        "    \"\"\"\n",
        "    Find the optimal weights vector for the binary classifier based on\n",
        "    the input features matrix and their true class labels.\n",
        "\n",
        "    Parameters:\n",
        "        train_matrix: List of feature vectors for the training data to be used\n",
        "            in finding optimal weights.\n",
        "        labels: List of labels corresponding to the feature vectors.\n",
        "\n",
        "    Returns:\n",
        "        w: The optimal weights vector for predicting the class label for\n",
        "            an input.\n",
        "    \"\"\"\n",
        "\n",
        "    w = np.zeros(train_matrix.shape[1])\n",
        "\n",
        "    for i in range(train_matrix.shape[0]):\n",
        "        y_hat = w.dot(train_matrix[i])\n",
        "        if y_hat >= 0:\n",
        "            if labels[i] == -1:\n",
        "                w -= train_matrix[i]\n",
        "        else:\n",
        "            if labels[i] == 1:\n",
        "                w += train_matrix[i]\n",
        "    return w\n",
        "\n",
        "def make_predictions(test_matrix, weight_vectors):\n",
        "    \"\"\"\n",
        "    Uses test data on generated binary classifiers to predict classification.\n",
        "\n",
        "    Parameters:\n",
        "        test_matrix: List of feature vectors for test data.\n",
        "        weight_vectors: List of weight vectors, each corresponding to a binary\n",
        "            classifier for a different class.\n",
        "\n",
        "    Returns:\n",
        "        final_y_hats: List of vectors of predicted labels, one list for each\n",
        "            binary classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    final_y_hats = list()\n",
        "\n",
        "    for test in test_matrix:\n",
        "\n",
        "        # computes dot product of weights vector and features vector for\n",
        "        # each binary classifier\n",
        "        potential_preds = [weights.dot(test) for weights in weight_vectors]\n",
        "        # chooses the label that produced the highest dot product value\n",
        "        # as the prediction for this feature vector\n",
        "        pred_label = np.where(potential_preds == np.max(potential_preds))[0][0]\n",
        "        # add predicted label to list of all predictions for the test data\n",
        "        final_y_hats.append(pred_label)\n",
        "\n",
        "    return final_y_hats\n",
        "\n",
        "def accuracy(pred_labels, true_labels):\n",
        "    \"\"\"\n",
        "    Calculates accuracy of a binary classifier.\n",
        "\n",
        "    Parameters:\n",
        "        pred_labels: List of label predictions from some binary classifier.\n",
        "        true_labels: List of the true labels for input data, the indices\n",
        "            of pred_labels and true_labels corresponding to respective\n",
        "            input images.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of predicted labels in comparison to true labels,\n",
        "            represented as a float between 0 and 1.\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    for pred_label, true_label in zip(pred_labels, true_labels):\n",
        "        if pred_label == true_label:\n",
        "            num_correct += 1\n",
        "    return (num_correct / len(true_labels))\n",
        "\n",
        "def one_vs_rest(data=data):\n",
        "    \"\"\"\n",
        "    Performs the one-vs-rest approach of binary classification on a data set.\n",
        "\n",
        "    Parameters:\n",
        "        data: Data set of images represented as a dictionary.\n",
        "\n",
        "    Returns:\n",
        "        table: A list summarizing the results of binary classification\n",
        "            by linear regression tactics based on image size, to be converted\n",
        "            to a DataFrame for clean output as a table.\n",
        "        data_sets: Dictionary of split data sets with corresponding image sizes\n",
        "            so that both OVR and OVO can act on the same data sets.\n",
        "    \"\"\"\n",
        "    sizes = [28, 14, 7, 4, 2]\n",
        "    table = []\n",
        "    data_sets = {}\n",
        "\n",
        "    # perform binary classification on each designated image size\n",
        "\n",
        "    for s in sizes:\n",
        "\n",
        "        # split the data\n",
        "        training_set, validation_set, test_set = data_split(data, s)\n",
        "\n",
        "        # save this data set in a dictionary so we can use it again in\n",
        "        # the one_vs_one approach\n",
        "\n",
        "        data_sets[s] = (deepcopy(training_set), deepcopy(validation_set),\n",
        "                        deepcopy(test_set))\n",
        "\n",
        "        # shuffle each set one last time\n",
        "        random.shuffle(training_set)\n",
        "        random.shuffle(validation_set)\n",
        "        random.shuffle(test_set)\n",
        "\n",
        "        # separate the feature vectors and class labels\n",
        "        train_feat = np.array([t[0].flatten() for t in training_set])\n",
        "        train_labels = np.array([t[1] for t in training_set])\n",
        "        val_feat = np.array([t[0].flatten() for t in validation_set])\n",
        "        val_labels = np.array([t[1] for t in validation_set])\n",
        "        test_feat = np.array([t[0].flatten() for t in test_set])\n",
        "        test_labels = np.array([t[1] for t in test_set])\n",
        "\n",
        "        # No hyperparameters need to be explored for linear regression, so we'll\n",
        "        # remerge the training and validation sets before starting training.\n",
        "        merged_features = np.append(train_feat, val_feat, axis=0)\n",
        "        merged_labels = np.append(train_labels, val_labels, axis=0)\n",
        "\n",
        "        # creating label vectors for each class to be used in their\n",
        "        # binary classifiers\n",
        "        y_zero = np.array([1 if label == 0 else -1 for label in merged_labels])\n",
        "        y_one = np.array([1 if label == 1 else -1 for label in merged_labels])\n",
        "        y_two = np.array([1 if label == 2 else -1 for label in merged_labels])\n",
        "        y_three = np.array([1 if label == 3 else -1 for label in merged_labels])\n",
        "        y_four = np.array([1 if label == 4 else -1 for label in merged_labels])\n",
        "        y_five = np.array([1 if label == 5 else -1 for label in merged_labels])\n",
        "        y_six = np.array([1 if label == 6 else -1 for label in merged_labels])\n",
        "        y_seven = np.array([1 if label == 7 else -1 for label in merged_labels])\n",
        "        y_eight = np.array([1 if label == 8 else -1 for label in merged_labels])\n",
        "        y_nine = np.array([1 if label == 9 else -1 for label in merged_labels])\n",
        "        label_vectors = [y_zero, y_one, y_two, y_three, y_four, y_five, y_six,\n",
        "                         y_seven, y_eight, y_nine]\n",
        "\n",
        "        # get optimal weight vectors for each binary classifier\n",
        "        all_weights = [get_weight_vectors(merged_features, labels) for\n",
        "                       labels in label_vectors]\n",
        "        # weights = get_weight_vectors(merged_features, label_vectors)\n",
        "        # test these weight vectors by making predictions for the test data\n",
        "        y_hats = make_predictions(test_feat, all_weights)\n",
        "        # calculate accuracy of these predictions\n",
        "        test_accuracy = accuracy(y_hats, test_labels)\n",
        "        # add results to table\n",
        "        table.append({\n",
        "            \"Approach\": \"One-VS-Rest\",\n",
        "            \"Image Size\": f\"{s}x{s}\",\n",
        "            \"Test Accuracy\": \"{:0.2f}%\".format(test_accuracy*100)\n",
        "        })\n",
        "\n",
        "    return table, data_sets\n",
        "\n",
        "def make_predictions_OVO(test_data, weights_dict):\n",
        "    \"\"\"\n",
        "     Uses test data on generated binary classifiers to predict classification,\n",
        "     following different process for the one-versus-one approach.\n",
        "\n",
        "    Parameters:\n",
        "        test_matrix: List of feature vectors for test data.\n",
        "        weights_dict: Dictionary of weight vectors with corresponding tuples\n",
        "            of the two classes on which the weight vector was based.\n",
        "\n",
        "    Returns:\n",
        "        predictions: List of vectors of predicted labels, one list for each\n",
        "            binary classifier.\n",
        "    \"\"\"\n",
        "    predictions = list()\n",
        "    for t in test_data:\n",
        "        # keep count of how many times each class is predicted\n",
        "        counts = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "        for classes, vector in zip(weights_dict.keys(), weights_dict.values()):\n",
        "            output = vector.dot(t)\n",
        "            if output > 0:\n",
        "                pred = classes[0]\n",
        "            else:\n",
        "                pred = classes[1]\n",
        "            counts[pred] += 1\n",
        "        # choose the class that was predicted the most as the final prediction\n",
        "        # for this feature vector\n",
        "        predictions.append(max(counts, key=counts.get))\n",
        "    return predictions\n",
        "\n",
        "def one_vs_one(data_sets):\n",
        "    \"\"\"\n",
        "    Performs the one-vs-one approach of binary classification on given\n",
        "    data sets.\n",
        "\n",
        "    Parameters:\n",
        "        data_sets: Dictionary of pre-split data sets that were created\n",
        "        during the One-Versus-Rest approach (to keep data as consistent\n",
        "        as possible between the two approaches so we can compare them).\n",
        "\n",
        "    Returns:\n",
        "        table: A list summarizing the results of binary classification\n",
        "            by linear regression tactics based on image size, to be converted\n",
        "            to a DataFrame for clean output as a table.\n",
        "    \"\"\"\n",
        "    table = []\n",
        "\n",
        "    # iterate over the sizes and corresponding data sets of each previously\n",
        "    # generated set from the already-run OVR approach\n",
        "    for size, data_set in zip(data_sets.keys(), data_sets.values()):\n",
        "\n",
        "        training_data, validation_data, test_data = data_set\n",
        "\n",
        "        # the training and validation sets are merged together given there\n",
        "        # are no hyperparameters to explore; we then separate them\n",
        "        # by class for one-vs-one implementation\n",
        "        class_zero = training_data[:80] + validation_data[:10]\n",
        "        class_one = training_data[80:160] + validation_data[10:20]\n",
        "        class_two = training_data[160:240] + validation_data[20:30]\n",
        "        class_three = training_data[240:320] + validation_data[30:40]\n",
        "        class_four = training_data[320:400] + validation_data[40:50]\n",
        "        class_five = training_data[400:480] + validation_data[50:60]\n",
        "        class_six = training_data[480:560] + validation_data[60:70]\n",
        "        class_seven = training_data[560:640] + validation_data[70:80]\n",
        "        class_eight = training_data[640:720] + validation_data[80:90]\n",
        "        class_nine = training_data[720:] + validation_data[90:]\n",
        "        classes = [class_zero, class_one, class_two, class_three,\n",
        "                    class_four, class_five, class_six, class_seven,\n",
        "                    class_eight, class_nine]\n",
        "\n",
        "        data_sets = list()\n",
        "\n",
        "        # we need to generate classifiers for every pair of datasets, so we\n",
        "        # create training sets by merging every possible pair of datasets\n",
        "\n",
        "        for i in range(9):\n",
        "            for j in range(i+1, 10):\n",
        "                data_sets.append(classes[i] + classes[j])\n",
        "\n",
        "        # will store optimal weight vectors from each classifier in a\n",
        "        # dictionary, where the vectors are the values and the keys are tuples\n",
        "        # where the first element is the class of the \"positive\" label for\n",
        "        # that weight vector's corresponding classifier and the second element\n",
        "        # is the class of the \"negative\" label\n",
        "        weights_dict = {}\n",
        "\n",
        "\n",
        "        # iterate over every data set to produce the weight vectors for each\n",
        "        # classifier\n",
        "        for data_set in data_sets:\n",
        "            # retain what classes are being treated as the positive and\n",
        "            # negative labels for this classifer\n",
        "            pos_label = data_set[0][1]\n",
        "            neg_label = data_set[-1][1]\n",
        "\n",
        "            # shuffle the training set for randomness in weight optimization\n",
        "            # function\n",
        "            random.shuffle(data_set)\n",
        "\n",
        "            # split the data set into feature and label vectors, then create\n",
        "            # a new label vector that makes all the pos_label labels 1, and\n",
        "            # all the neg_label labels -1\n",
        "            features = np.array([e[0].flatten() for e in data_set])\n",
        "            labels = np.array([e[1] for e in data_set])\n",
        "            true_labels = np.array([1 if label == pos_label else -1 for label\n",
        "                                    in labels])\n",
        "\n",
        "            # find the optimal weight vector and add it to the dictionary\n",
        "            # along with the positive and negative class labels\n",
        "            weight_vector = get_weight_vectors(features, true_labels)\n",
        "            weights_dict[(pos_label, neg_label)] = weight_vector\n",
        "\n",
        "        # shuffle the test set before testing\n",
        "        random.shuffle(test_data)\n",
        "        # split into feature and label vectors\n",
        "        test_features = np.array([e[0].flatten() for e in test_data])\n",
        "        test_labels = np.array([e[1] for e in test_data])\n",
        "        # make predictions for each instance in the test set\n",
        "        final_predictions = make_predictions_OVO(test_features, weights_dict)\n",
        "        # record accuracy\n",
        "        test_accuracy = accuracy(final_predictions, test_labels)\n",
        "        # add results to table\n",
        "        table.append({\n",
        "            \"Approach\": \"One-VS-One\",\n",
        "            \"Image Size\": f\"{size}x{size}\",\n",
        "            \"Test Accuracy\": \"{:0.2f}%\".format(test_accuracy*100)\n",
        "        })\n",
        "    return table\n",
        "\n",
        "table_OVR, split_sets = one_vs_rest()\n",
        "table_OVO = one_vs_one(split_sets)\n",
        "full_table = pd.concat([pd.DataFrame(table_OVR), pd.DataFrame(table_OVO)],\n",
        "                       ignore_index=True, sort=False)\n",
        "print(full_table.to_string(index=False))"
      ],
      "metadata": {
        "id": "ueSohSbyCljy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4edc593c-1038-4aa7-9cd2-9bad5da6af6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Approach Image Size Test Accuracy\n",
            "One-VS-Rest      28x28        78.00%\n",
            "One-VS-Rest      14x14        74.00%\n",
            "One-VS-Rest        7x7        67.00%\n",
            "One-VS-Rest        4x4        57.00%\n",
            "One-VS-Rest        2x2        27.00%\n",
            " One-VS-One      28x28        73.00%\n",
            " One-VS-One      14x14        80.00%\n",
            " One-VS-One        7x7        78.00%\n",
            " One-VS-One        4x4        61.00%\n",
            " One-VS-One        2x2        20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some interesting trends were noted after running the linear regression classifier for multiple random seeds. Firstly, both the OVR and OVO approaches had very similar accuracies - neither approach seemed to have a greater performance than the other. It was observed that the 28x28, 14x14, and 7x7 image sizes consistently had the best test accuracy in comparison to 4x4 and 2x2, which makes sense considering that they have more features and thus more information to be used in classification. However, by the same token, classification by linear regression performs better on input of lower dimensions, so 28x28 often performed equally as well if not slightly worse than 14x14 and 7x7. It can be concluded that there's some balance between lots of features and not-too-high dimensions that come into play in order for classification by linear regression/binary perceptron to perform well."
      ],
      "metadata": {
        "id": "Ey2x0HCILkMb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_6z07BdyqTw"
      },
      "source": [
        "---\n",
        "## Task 2: Data Augmentation [20 points]\n",
        "\n",
        "Your boss was unhappy with the test accuracy, especially of your 2x2 image classifier, and has made some suggestions. The problem, according to your boss, is that there is not enough data in each input $x$. You are told to augment the data with derived features in order to help the classifier.\n",
        "\n",
        "Specifically, given an input $x$, create additional attributes by computing all of the data up to powers of two. For example, in the 2x2 case your example $x$ consists of four pixel values $x_0$, $x_1$, $x_2$, and $x_3$. Your new input data would have:\n",
        "\n",
        "* all power of zero: 1 (constant)\n",
        "* all powers of one: $x_0$, $x_1$, $x_2$, $x_3$\n",
        "* all powers of two:\n",
        "\n",
        "  $x_0^2$, $x_0 x_1$, $x_0 x_2$, $x_0 x_3$,\n",
        "  \n",
        "  $x_1^2$, $x_1 x_2$, $ x_1 x_3$,\n",
        "  \n",
        "  $x_2^2$, $x_2 x_3$,\n",
        "  \n",
        "  $x_3^2$\n",
        "\n",
        "The data would have 15 values, which has the potential to learn nonlinear relationships between the original inputs, which was not possible before.\n",
        "\n",
        "### Report Results\n",
        "\n",
        "Report the test accuracy for OvR only, with the data augmentation approach, for each of the input image sizes, 28x28, 14x14, 7x7, 4x4, 2x2 (again, perhaps incorporating a table). Report any interesting results or observations.\n",
        "\n",
        "Also, explain to your boss what the danger is of looking at a model's final test accuracy and then suggesting changes to improve it. What should be done instead, if you know you will consider different types of models or hyperparameters in the same model class?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did not include 28x28 in our data augmentation methods, which we will explain below the code block."
      ],
      "metadata": {
        "id": "9t0wwwrKOFkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCvHIrBwyqTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80968f7e-699e-491a-9369-7cfa2f3d312f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Size Test Accuracy\n",
            "     14x14        89.00%\n",
            "       7x7        81.00%\n",
            "       4x4        48.00%\n",
            "       2x2        11.00%\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "random.seed(20)\n",
        "\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "def augment_features(vector):\n",
        "    \"\"\"\n",
        "    Augments the data of the input vector by generating a new one that consists\n",
        "    of all the data up to powers of two.\n",
        "\n",
        "    Parameters:\n",
        "        vector: Original features vector\n",
        "\n",
        "    Returns:\n",
        "        aug_vector: new features vector of augmented data. For original vector\n",
        "            of length n, aug_vector will have a length of n(n+1)/2 + n + 1.\n",
        "    \"\"\"\n",
        "    # start augmented data vector with data powered to 0 (1, always true)\n",
        "    aug_vector = np.ones(1)\n",
        "    # add all the data to powers of one (original vector)\n",
        "    aug_vector = np.concatenate((aug_vector, vector))\n",
        "    length = len(aug_vector)\n",
        "    # add all the data to powers of two\n",
        "    for i in range(1, length):\n",
        "        for j in range(i, length):\n",
        "            aug_vector = np.append(aug_vector, aug_vector[i] * aug_vector[j])\n",
        "\n",
        "    return aug_vector\n",
        "\n",
        "def augmented_OVR(data=data):\n",
        "    \"\"\"\n",
        "    Alternate One-vs-Rest multiclass classifier that first augments the data\n",
        "    in the feature vectors.\n",
        "\n",
        "    Parameters:\n",
        "        data_sets: Dictionary of pre-split data sets that were created\n",
        "        during the One-Versus-Rest approach (to keep data as consistent\n",
        "        as possible between the two approaches so we can compare them).\n",
        "\n",
        "    Returns:\n",
        "        table: A list summarizing the results of binary classification\n",
        "            by linear regression tactics based on image size, to be converted\n",
        "            to a DataFrame for clean output as a table.\n",
        "    \"\"\"\n",
        "    sizes = [14, 7, 4, 2]\n",
        "    table = []\n",
        "\n",
        "    # perform classification on each image size\n",
        "\n",
        "    for s in sizes:\n",
        "\n",
        "        # split the data\n",
        "        train_set, valid_set, test_set = data_split(data, s)\n",
        "\n",
        "        # shuffle each data set one last time\n",
        "        random.shuffle(train_set)\n",
        "        random.shuffle(valid_set)\n",
        "        random.shuffle(test_set)\n",
        "\n",
        "\n",
        "        # split the data into feature vectors and labels\n",
        "        train_feat = np.array([t[0].flatten() for t in train_set])\n",
        "        train_labels = np.array([t[1] for t in train_set])\n",
        "        val_feat = np.array([t[0].flatten() for t in valid_set])\n",
        "        val_labels = np.array([t[1] for t in valid_set])\n",
        "        test_feat = np.array([t[0].flatten() for t in test_set])\n",
        "        test_labels = np.array([t[1] for t in test_set])\n",
        "\n",
        "        # since there are no hyperparameters to explore for linear regression,\n",
        "        # we remerge the training and validation sets to use for training,\n",
        "        # while also augmenting the features of the training and test sets\n",
        "        merged_features = np.append(train_feat, val_feat, axis=0)\n",
        "        merged_features = np.array([augment_features(t) for t in\n",
        "                                    merged_features])\n",
        "        test_feat = np.array([augment_features(t) for t in test_feat])\n",
        "        merged_labels = np.append(train_labels, val_labels, axis=0)\n",
        "\n",
        "        y_zero = np.array([1 if label == 0 else -1 for label in merged_labels])\n",
        "        y_one = np.array([1 if label == 1 else -1 for label in merged_labels])\n",
        "        y_two = np.array([1 if label == 2 else -1 for label in merged_labels])\n",
        "        y_three = np.array([1 if label == 3 else -1 for label in merged_labels])\n",
        "        y_four = np.array([1 if label == 4 else -1 for label in merged_labels])\n",
        "        y_five = np.array([1 if label == 5 else -1 for label in merged_labels])\n",
        "        y_six = np.array([1 if label == 6 else -1 for label in merged_labels])\n",
        "        y_seven = np.array([1 if label == 7 else -1 for label in merged_labels])\n",
        "        y_eight = np.array([1 if label == 8 else -1 for label in merged_labels])\n",
        "        y_nine = np.array([1 if label == 9 else -1 for label in merged_labels])\n",
        "        label_vectors = [y_zero, y_one, y_two, y_three, y_four, y_five, y_six,\n",
        "                         y_seven, y_eight, y_nine]\n",
        "\n",
        "        # get optimal weight vectors for each class' binary classifier\n",
        "        all_weights = [get_weight_vectors(merged_features, labels) for labels\n",
        "                       in label_vectors]\n",
        "\n",
        "        # test these weight vectors by making predictions for the test data\n",
        "        y_hats = make_predictions(test_feat, all_weights)\n",
        "\n",
        "        # calculate accuracy of these predictions\n",
        "        test_accuracy = accuracy(y_hats, test_labels)\n",
        "\n",
        "        # add results to the table\n",
        "        table.append({\n",
        "            \"Image Size\": f\"{s}x{s}\",\n",
        "            \"Test Accuracy\": \"{:0.2f}%\".format(test_accuracy*100)\n",
        "        })\n",
        "\n",
        "    return table\n",
        "\n",
        "t = augmented_OVR()\n",
        "print(pd.DataFrame(t).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to our first attempts at classification in task 1, we can see that greater image sizes came with a higher test accuracy, as expected. 14x14 specifically seemed to perform slightly better with data augmentation, although generally the performances did not improve much (although they did not worsen either - performance was about the same overall).\n",
        "\n",
        "We left 28x28 out of our analysis. Using data augmentation, feature vectors would require the model to have **308,505** parameters. This clearly not only leads to extreme overfitting, but the run time became abysmal. The 14x14 image size already had a run time of several minutes with only 19,503 parameters, so attempting to create a model with nearly **16 times** as many parameters would exponentially increase runtime.\n",
        "\n",
        "As stated previously, this type of classification does not perform as well in higher dimensions. Although the added features provided much information about the relationship between elements of the input, it is still increasing the number of parameters by quite a bit. So, again, there is a balance between the two that it appears 14x14 achieves quite well. However, as explained in the previous paragraph, 28x28 is simply too high-dimensional.\n",
        "\n",
        "Using test accuracy to measure model performance should not be practiced because this then means that you are using the test data to make decisions about the model, while test data should only be used as unseen data to analyze the generalization prowess of the model. If one knows that they are going to consider multiple models or multiple hyperparameters within a model, the user should implement **validation data** in a training-validation-test split. Create different models solely using the training data, then run them on the validation data to determine which one is best. From there, you can see how it generalizes using the test data and test accuracy."
      ],
      "metadata": {
        "id": "npDOiopeOPTL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz7hMCAkyqTw"
      },
      "source": [
        "---\n",
        "## Task 3: k-Nearest Neighbors Classifier [20 points]\n",
        "\n",
        "Your boss is still unhappy with the results (and still ignoring your advice about not using test data accuracy for model decisions).\n",
        "\n",
        "Next, you are to use the k-nearest neighbors approach to build a classifier for our data. Since we have multiple classes, the one that gets selected can be based on a plurality vote of the $k$ closest samples (whichever category is most frequent). If there are ties, select the class based on the sum of the distances from the test point. For example, if $k=5$, and the closest 5 samples have two pictures that are from category \"1\" and two pictures that are from category \"7\", then you choose the output by computing the sum of the distance from the test point and the two \"5\" samples, as well as the sum of distances from the test point to the two \"7\" samples, and then outputting the class with the smaller total distance.\n",
        "\n",
        "### Report Results\n",
        "\n",
        "For each image size, exhaustively explore different values of $k$ up to 50. Report the best test accuracy. Report the average time taken to do a lookup with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7pGs5acyqTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376a5709-caba-485c-b0ce-5ccf9875f0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Size # of Neigbors Best Test Accuracy     Avg. Lookup Time\n",
            "     28x28             4             87.00% 0.03160 milliseconds\n",
            "     14x14             3             88.00% 0.03178 milliseconds\n",
            "       7x7             5             92.00% 0.02363 milliseconds\n",
            "       4x4             3             73.00% 0.01873 milliseconds\n",
            "       2x2            10             45.00% 0.01733 milliseconds\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "import time\n",
        "random.seed(0)\n",
        "\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "def euclid_dist(vec_1, vec_2):\n",
        "    \"\"\"\n",
        "    We used euclidean distance to measure distance between\n",
        "    vectors for the nearest neighbor computations.\n",
        "\n",
        "    Parameters:\n",
        "        vec_1: The first vector to be considered.\n",
        "        vec_2: The second vector of which the euclidean distance\n",
        "            between itself and vec_1 will be calculated.\n",
        "\n",
        "    Returns:\n",
        "        float: The euclidean distance between vec_1 and vec_2.\n",
        "    \"\"\"\n",
        "    dist = 0\n",
        "\n",
        "    for feat_1, feat_2 in zip(vec_1, vec_2):\n",
        "        dist += (feat_1 - feat_2)**2\n",
        "\n",
        "    return dist**0.5\n",
        "\n",
        "def make_prediction_KNN(dist_dict):\n",
        "    \"\"\"\n",
        "    Takes in a dictionary of the nearest neighbors for some vector,\n",
        "    and returns the label that appears the most between the neighbors\n",
        "    (returning the label with the shortest total distance from the test\n",
        "    point if there are ties).\n",
        "\n",
        "    Parameters:\n",
        "        dist_dict: Dictionary of previously determined nearest neighbors,\n",
        "            which has the distances as keys and their respective class labels\n",
        "            as the values.\n",
        "    \"\"\"\n",
        "    # keep track of how many times each class appears in the dictionary\n",
        "    # of previously determined nearest neighbors\n",
        "    counts = {\n",
        "        0:0,\n",
        "        1:0,\n",
        "        2:0,\n",
        "        3:0,\n",
        "        4:0,\n",
        "        5:0,\n",
        "        6:0,\n",
        "        7:0,\n",
        "        8:0,\n",
        "        9:0\n",
        "    }\n",
        "\n",
        "    # increments classes that appear in nearest neighbors dictionary\n",
        "    for label in dist_dict.values():\n",
        "        counts[label] += 1\n",
        "\n",
        "    # place holder variables to help with determining most frequent\n",
        "    # label and presence of ties\n",
        "    tie_flag = False\n",
        "    ties = list()\n",
        "    max = 0\n",
        "    max_label = 0\n",
        "\n",
        "    for c in counts.keys():\n",
        "\n",
        "        # found a label more occurrent than the current most-occurrent label\n",
        "        if counts[c] > max:\n",
        "            max = counts[c]\n",
        "            # keep track is currently most occurrent for breaking ties if\n",
        "            # needed\n",
        "            max_label = c\n",
        "            # if the previously most occurrent label was a tie, we can\n",
        "            # ignore that now that we've found a more occurrent label\n",
        "            if tie_flag:\n",
        "                tie_flag = False\n",
        "\n",
        "        # if we found a tie with the current most occurrent label (ignores\n",
        "        # 0, as this is what we initialize the placeholder variable as)\n",
        "        elif counts[c] == max and counts[c] != 0:\n",
        "            tie_flag = True\n",
        "            # store the two labels that tied in a tuple for tie-breaking\n",
        "            ties.append(max_label)\n",
        "            ties.append(c)\n",
        "\n",
        "    # if there were no ties, return the most occurrent label\n",
        "    if not tie_flag:\n",
        "        return max_label\n",
        "\n",
        "    # if a tie occurred\n",
        "    else:\n",
        "\n",
        "        sum_1 = 0\n",
        "        sum_2 = 0\n",
        "\n",
        "        # calculate the total sum of the distances of neighbors with\n",
        "        # each of the class labels that tied\n",
        "        for key in dist_dict.keys():\n",
        "            if dist_dict[key] == ties[0]:\n",
        "                sum_1 += key\n",
        "            elif dist_dict[key] == ties[1]:\n",
        "                sum_2 += key\n",
        "\n",
        "        # return the label with the lowest sum\n",
        "        return ties[0] if sum_1 < sum_2 else ties[1]\n",
        "\n",
        "def lookup(data, data_labels, lookup_data, lookup_labels, k):\n",
        "    \"\"\"\n",
        "    Function that runs the table lookup over the whole data set\n",
        "    and returns the accuracy, along with the total time taken\n",
        "    for all lookups.\n",
        "\n",
        "    Parameters:\n",
        "        data: Set of feature vectors to be predicted for.\n",
        "        data_labels: True classes of the input feature vectors.\n",
        "        lookup_data: Training data that is used in lookup table construction.\n",
        "        lookup_labels: True classes of the training data.\n",
        "        k: Number of nearest neighbors to be determined.\n",
        "    \"\"\"\n",
        "\n",
        "    total_time = 0\n",
        "    correct = 0\n",
        "\n",
        "    # iterate over every feature vector in the input data\n",
        "    for i in range(len(data)):\n",
        "        d = data[i]\n",
        "        # will store the distances from every training data vector\n",
        "        # to the test point in a dictionary, with the distances as keys\n",
        "        # and the training vector's true class label as the value\n",
        "        distances = {}\n",
        "        for t, l in zip(lookup_data, lookup_labels):\n",
        "            distances[euclid_dist(d, t)] = l\n",
        "\n",
        "        # sort the keys in ascending order and retain the k vectors\n",
        "        # with the shortest distances\n",
        "        keys_sorted = sorted(distances.keys())[:k]\n",
        "\n",
        "        # construct new dictionary of these k nearest neighbors\n",
        "        nearest_dict = {}\n",
        "        for key in keys_sorted:\n",
        "            nearest_dict[key] = distances[key]\n",
        "        # time how long the prediction/table lookup takes\n",
        "        t0 = time.time()\n",
        "        # pass in the dictionary of nearest neighbors to determine\n",
        "        # what prediction will be made\n",
        "        pred = make_prediction_KNN(nearest_dict)\n",
        "        t1 = time.time()\n",
        "        total_time += (t1-t0)\n",
        "\n",
        "        # increment every time a label is predicted correctly in order\n",
        "        # to determine overall accuracy\n",
        "        if pred == data_labels[i]:\n",
        "            correct += 1\n",
        "\n",
        "    return (correct / len(data)), total_time\n",
        "\n",
        "\n",
        "def KNN(data=data):\n",
        "    \"\"\"\n",
        "    Runs a k-nearest neighbors classifcation algorithm on the given\n",
        "    data set over a range of image sizes and possible numbers of nearest\n",
        "    neighbors, returning a table summarizing the results. Validation data\n",
        "    is used to determine which value of k performed the best for each image\n",
        "    size, and those best k's are then used on the test data for that image size.\n",
        "    \"\"\"\n",
        "    # different k values that we tried\n",
        "    ks = [2, 3, 4, 5, 10, 15, 20, 25, 30, 40, 50]\n",
        "    sizes = [28, 14, 7, 4, 2]\n",
        "    table = []\n",
        "\n",
        "    for s in sizes:\n",
        "        tr, val, te = data_split(data, s)\n",
        "\n",
        "        random.shuffle(tr)\n",
        "        random.shuffle(val)\n",
        "        random.shuffle(te)\n",
        "\n",
        "        train_feat = np.array([t[0].flatten() for t in tr])\n",
        "        train_labels = np.array([t[1] for t in tr])\n",
        "        val_feat = np.array([t[0].flatten() for t in val])\n",
        "        val_labels = np.array([t[1] for t in val])\n",
        "        test_feat = np.array([t[0].flatten() for t in te])\n",
        "        test_labels = np.array([t[1] for t in te])\n",
        "\n",
        "        best_k = 0\n",
        "        accuracy = 0\n",
        "        # perform a table lookup using different values of k, keeping track\n",
        "        # of the value that returned the highest accuracy (in case of ties,\n",
        "        # return the larger value of k)\n",
        "        for k in ks:\n",
        "            acc, total_time = lookup(val_feat, val_labels, train_feat,\n",
        "                                     train_labels, k)\n",
        "            if acc >= accuracy:\n",
        "                best_k = k\n",
        "                accuracy = acc\n",
        "\n",
        "        test_acc, test_time = lookup(test_feat, test_labels, train_feat,\n",
        "                                     train_labels, best_k)\n",
        "        table.append({\n",
        "            \"Image Size\": f\"{s}x{s}\",\n",
        "            \"# of Neigbors\": f\"{best_k}\",\n",
        "            \"Best Test Accuracy\": \"{:0.2f}%\".format(test_acc*100),\n",
        "            \"Avg. Lookup Time\":\n",
        "            \"{:0.5f} milliseconds\".format(total_time/len(test_feat)*1e3)\n",
        "        })\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "t_KNN = KNN()\n",
        "print(pd.DataFrame(t_KNN).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to our previous models, we can see that k-nearest neighbors performed the best for image sizes 7x7 and above. However, we can see a great improvement in performance specifically for the 2x2 and 4x4 image sizes.\n",
        "\n",
        "We tested out a range of k values within the range of [2, 50] as seen in the `ks` list iterated over, and found that amongst all the image sizes, k values consistently brought lower test accuracies when increasing past 10. This makes sense, as the idea of balance is once again brought into play when it comes to k-nearest neighbors - small values of k underfit the data and large values of k overfit, so somewhere in the middle produces the best results. Here, values of 3-5 seemed to produce the best results (aside from 10 for the 2x2 size). The general trend was that of a concave-down hyperbola - accuracy improved slightly up to some value between 4 and 10, and then decreased."
      ],
      "metadata": {
        "id": "D01eQgI2RwGl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyN1oAczyqTw"
      },
      "source": [
        "---\n",
        "## Task 4: Neural Networks [40 Points]\n",
        "\n",
        "Next, your boss wants you to try neural networks. Rather than using a library for everything, you will **only** use `pytorch` to perform backpropagation and compute gradients. You can write your own neural network class if desired, don't use anything from `pytorch` for that.\n",
        "\n",
        "\n",
        "An example network and how to compute gradients with pytorch is shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVYHnm2fyqTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e15c061-7f7c-4a57-8827-058ff06a4b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Output: tensor([0.5348, 0.2167, 0.2485], grad_fn=<SoftmaxBackward0>)\n",
            "Desired Output: tensor([1., 0., 0.])\n",
            "Initial loss: 0.625852644443512\n",
            "Gradient for weights matrix W1: tensor([[-2.9431e-03, -5.8862e-03, -8.8293e-03],\n",
            "        [ 4.1993e-03,  8.3986e-03,  1.2598e-02],\n",
            "        [-3.0524e-06, -6.1048e-06, -9.1572e-06]])\n",
            "New loss after updating weights and biases: 0.6079817414283752\n"
          ]
        }
      ],
      "source": [
        "# Example of using pytorch to compute gradients and updates weights and biases\n",
        "#\n",
        "# The network consists of:\n",
        "# 1. An input layer with 3 features.\n",
        "# 2. A first hidden layer with 3 neurons. Each neuron in this layer performs a\n",
        "#    linear transformation\n",
        "#    on the input data using a weight matrix (W1) and a bias vector (b1). This\n",
        "#    is followed by a sigmoid activation function.\n",
        "# 3. A second hidden layer, also with 3 neurons, which processes the output of\n",
        "#    the first layer. Similar\n",
        "#    to the first layer, it uses a weight matrix (W2) and a bias vector (b2)\n",
        "#    for linear transformation,\n",
        "#    followed by a softmax activation function. The softmax activation is used\n",
        "#    here to normalize the\n",
        "#    output of the second layer into a probability distribution over the three\n",
        "#    classes. This is particularly\n",
        "#    useful for multi-class classification problems.\n",
        "# 4. The network uses cross-entropy as the loss function, which is a common\n",
        "#    choice for classification tasks\n",
        "#    involving softmax outputs. This loss function compares the predicted\n",
        "#    probability distribution with the\n",
        "#    true distribution (one-hot encoded) and penalizes the predictions that\n",
        "#    diverge from the actual labels.\n",
        "#\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# Initialize input, weights, and biases\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "W1 = torch.tensor([[0.1, 0.2, 0.5],\n",
        "                  [-0.1, -0.5, -1.1],\n",
        "                  [0, 7.5, -1.1]], requires_grad=True)\n",
        "b1 = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
        "\n",
        "W2 = torch.tensor([[0.1, -0.3, 0.4],\n",
        "                  [0.2, 0.4, -0.6],\n",
        "                  [-0.1, 0.5, -0.2]], requires_grad=True)\n",
        "b2 = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Target output\n",
        "y_true = torch.tensor([1.0, 0.0, 0.0])\n",
        "\n",
        "# Forward pass through first layer\n",
        "z1 = torch.matmul(W1, x) + b1\n",
        "a1 = torch.sigmoid(z1)  # Sigmoid activation\n",
        "\n",
        "# Forward pass through second layer\n",
        "z2 = torch.matmul(W2, a1) + b2\n",
        "a2 = torch.softmax(z2, dim=0)  # Softmax activation\n",
        "\n",
        "print(\"Initial Output:\", a2)\n",
        "print(\"Desired Output:\", y_true)\n",
        "\n",
        "# Compute loss (Cross-entropy): https://en.wikipedia.org/wiki/Cross-entropy\n",
        "loss = -torch.sum(y_true * torch.log(a2))\n",
        "print(\"Initial loss:\", loss.item())\n",
        "\n",
        "# Backpropagation\n",
        "loss.backward()\n",
        "\n",
        "# you can print out gradient for each element now\n",
        "print(\"Gradient for weights matrix W1:\", W1.grad)\n",
        "\n",
        "# Update weights and biases based on gradient (should reduce loss)\n",
        "learning_rate = 0.02\n",
        "\n",
        "# the no_grad() environment is needed to indicate that the computation should\n",
        "# not be part of the gradient computation\n",
        "with torch.no_grad():\n",
        "    W1 -= learning_rate * W1.grad\n",
        "    b1 -= learning_rate * b1.grad\n",
        "    W2 -= learning_rate * W2.grad\n",
        "    b2 -= learning_rate * b2.grad\n",
        "\n",
        "# After the update, clear the gradients (in case we want to compute them again\n",
        "# later)\n",
        "W1.grad.zero_()\n",
        "b1.grad.zero_()\n",
        "W2.grad.zero_()\n",
        "b2.grad.zero_()\n",
        "\n",
        "# Forward pass with updated weights and biases\n",
        "z1 = torch.matmul(W1, x) + b1\n",
        "a1 = torch.sigmoid(z1)  # Sigmoid activation\n",
        "z2 = torch.matmul(W2, a1) + b2\n",
        "a2 = torch.softmax(z2, dim=0)  # Softmax activation\n",
        "\n",
        "# Compute new loss\n",
        "new_loss = -torch.sum(y_true * torch.log(a2))\n",
        "print(\"New loss after updating weights and biases:\", new_loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above updates the parameters based on a single piece of data, but often multiple inputs are used and their gradient is averaged when updating a model.\n",
        "\n",
        "Your task is to write the training code for the different neural network architectures proposed and report accuracy. Start with all random parameters between -1 and 1. Training should stop when the accuracy, as measured on the validation data, no longer appears to be improving. You can plot the validation data accuracy over time to ensure this looks correct. If this takes too long but it appears the model is still improving in accuracy, consider increasing the learning rate (start with 0.02 as in the example).\n",
        "\n",
        "For the gradient, you are to compute the gradient over the full set of training data, and then average them together before you update. Then, repeat with mini-batches of size 100, with 10 random samples from each class. This should update the model weights faster, but may require more updates to get the accuracy down.\n",
        "\n",
        "### Report Results\n",
        "\n",
        "Provide at least one plot of your validation data accuracy going down over time as training progresses. What was the condition you decided to use to detect if training should stop? How many updates were needed in the case of your plot?\n",
        "\n",
        "\n",
        "Create a table where each row corresponds to one model and training method (mini-batch or full). Use the 7x7 version of the data (49-dimensional inputs). You are to explore the following models: the number of hidden layers can be varied between 2 and 4. Each layer's size can be 16, 32, or 64 neurons (all hidden layers have the same number of neurons). Explore three different activation functions for the network, ReLU (`torch.relu`), arctan (`torch.atan`), and sigmoid (`torch.sigmoid`). After the final layer, use a softmax rather than the normal network activation function, to ensure all outputs are between 0 and 1. There should be 10 outputs, one for each class in the MNIST data.\n",
        "\n",
        "In the table, report the architecture, training time, number of model updates and test accuracy. What is the best architecture? Did mini-batches help with anything? Report any other interesting observations.\n",
        "\n"
      ],
      "metadata": {
        "id": "hddWtjp7bHvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "!pip install matplotlib torchvision torch"
      ],
      "metadata": {
        "id": "3Z1UD3w7MhkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9e08f1-8a4e-47f1-b83d-10c53c20274c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Activation Functions\n",
        "def relu(x, derivative=False):\n",
        "    if derivative:\n",
        "        return (x > 0).astype(float)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x, derivative=False):\n",
        "    if derivative:\n",
        "        return sigmoid(x) * (1 - sigmoid(x))\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def arctan(x, derivative=False):\n",
        "    if derivative:\n",
        "        return 1 / (1 + x ** 2)\n",
        "    return np.arctan(x)\n",
        "\n",
        "activation_functions = {\n",
        "    'ReLU': relu,\n",
        "    'Sigmoid': sigmoid,\n",
        "    'Arctan': arctan\n",
        "}\n",
        "\n",
        "# Load Mini-MNIST data\n",
        "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# split data and flatten out the image matrices into feature vectors\n",
        "train_data, val_data, test_data = data_split(data, 7)\n",
        "train_data = [(t[0].flatten(), t[1]) for t in train_data]\n",
        "val_data = [(t[0].flatten(), t[1]) for t in val_data]\n",
        "test_data = [(t[0].flatten(), t[1]) for t in test_data]\n",
        "\n",
        "# randomly shuffle the datasets again (training data not getting\n",
        "# reorganized yet to easily make balanced batches if using minibatch)\n",
        "random.shuffle(val_data)\n",
        "random.shuffle(test_data)\n",
        "\n",
        "# Neural Network Class with Backpropagation\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_layers, hidden_size, output_size):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # First hidden layer\n",
        "        self.weights.append(np.random.uniform(-1, 1, (input_size, hidden_size)))\n",
        "        self.biases.append(np.random.uniform(-1, 1, (hidden_size,)))\n",
        "\n",
        "        # Additional hidden layers\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            self.weights.append(np.random.uniform(-1, 1, (hidden_size,\n",
        "                                                          hidden_size)))\n",
        "            self.biases.append(np.random.uniform(-1, 1, (hidden_size,)))\n",
        "\n",
        "        # Output layer\n",
        "        self.weights.append(np.random.uniform(-1, 1, (hidden_size,\n",
        "                                                      output_size)))\n",
        "        self.biases.append(np.random.uniform(-1, 1, (output_size,)))\n",
        "\n",
        "    def forward(self, x, activation_func):\n",
        "        a = x\n",
        "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            a = activation_func(np.dot(a, w) + b)\n",
        "\n",
        "        # Output layer with softmax\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        return torch.nn.functional.softmax(torch.tensor(z), dim=0)\n",
        "\n",
        "    def compute_gradients(self, data, activation_func):\n",
        "        grads = [np.zeros_like(w) for w in self.weights]\n",
        "        biases_grads = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        for features, label in data:\n",
        "            # Forward pass\n",
        "            activations = [features]\n",
        "\n",
        "            x = features\n",
        "            for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "                x = activation_func(np.dot(x, w) + b)\n",
        "                activations.append(x)\n",
        "\n",
        "            # Output layer\n",
        "            z = np.dot(x, self.weights[-1]) + self.biases[-1]\n",
        "            output = torch.nn.functional.softmax(torch.tensor(z),\n",
        "                                                 dim=0).detach().numpy()\n",
        "\n",
        "            # Backward pass\n",
        "            delta = output\n",
        "            delta[label] -= 1\n",
        "\n",
        "            grads[-1] = np.outer(activations[-2], delta)\n",
        "            biases_grads[-1] = delta\n",
        "\n",
        "            # Backpropagation through hidden layers\n",
        "            for i in range(len(self.weights) - 2, -1, -1):\n",
        "                derivative = activation_func(activations[i + 1],\n",
        "                                             derivative=True)\n",
        "                delta = np.dot(delta, self.weights[i + 1].T) * derivative\n",
        "\n",
        "                grads[i] = np.outer(activations[i], delta)\n",
        "                biases_grads[i] = delta.mean(axis=0)\n",
        "\n",
        "        return grads, biases_grads\n",
        "\n",
        "    def update_weights(self, grads, biases_grads, learning_rate):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * grads[i]\n",
        "            self.biases[i] -= learning_rate * biases_grads[i]\n",
        "\n",
        "# Accuracy Calculation\n",
        "def compute_accuracy(model, data, activation_func):\n",
        "    correct = 0\n",
        "    for features, label in data:\n",
        "        output = model.forward(features, activation_func).numpy()\n",
        "        predicted_label = np.argmax(output)\n",
        "        if predicted_label == label:\n",
        "            correct += 1\n",
        "    return correct / len(data)\n",
        "\n",
        "\n",
        "# function for generating 8 100-image batches\n",
        "def get_batches(train_data):\n",
        "    classes = []\n",
        "    for i in range(10):\n",
        "        c = train_data[i*80:(i+1)*80]\n",
        "        classes.append(c)\n",
        "    batches = []\n",
        "    num_batches = 8\n",
        "    batch_size = 100\n",
        "    for j in range(num_batches):\n",
        "        b = []\n",
        "        for c in classes:\n",
        "            b1 = c[j*10:(j+1)*10]\n",
        "            b = b + b1\n",
        "        random.shuffle(b)\n",
        "        batches.append(b)\n",
        "    return batches\n",
        "\n",
        "# Training Function with Tracking\n",
        "def train(model, train_data, val_data, learning_rate, activation_func,\n",
        "          mini_batches, stop_condition):\n",
        "    best_val_accuracy = 0\n",
        "    no_improvement = 0\n",
        "    epoch = 0\n",
        "    accuracies = []\n",
        "    model_updates = 0  # To track the number of model updates\n",
        "\n",
        "    while no_improvement < stop_condition:\n",
        "        epoch += 1\n",
        "\n",
        "        if mini_batches:\n",
        "\n",
        "            batches = get_batches(train_data)\n",
        "            for batch in batches:\n",
        "                grads, biases_grads = model.compute_gradients(batch,\n",
        "                                                              activation_func)\n",
        "                model.update_weights(grads, biases_grads, learning_rate)\n",
        "                model_updates += 1\n",
        "        else:\n",
        "            random.shuffle(train_data)\n",
        "            grads, biases_grads = model.compute_gradients(train_data,\n",
        "                                                          activation_func)\n",
        "            model.update_weights(grads, biases_grads, learning_rate)\n",
        "            model_updates += 1\n",
        "\n",
        "        # Validate accuracy\n",
        "        val_accuracy = compute_accuracy(model, val_data, activation_func)\n",
        "        accuracies.append(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "    return best_val_accuracy, epoch, model_updates, accuracies\n",
        "\n",
        "# Model parameters\n",
        "input_size = 7 * 7  # 49-dimensional inputs for 7x7\n",
        "output_size = 10\n",
        "learning_rate = 0.02\n",
        "stop_condition = 5\n",
        "\n",
        "# Define architectures and training methods as outlined in assignment\n",
        "# description\n",
        "architectures = [\n",
        "    {'hidden_layers': 2, 'hidden_size': 16},\n",
        "    {'hidden_layers': 2, 'hidden_size': 32},\n",
        "    {'hidden_layers': 2, 'hidden_size': 64},\n",
        "    {'hidden_layers': 3, 'hidden_size': 16},\n",
        "    {'hidden_layers': 3, 'hidden_size': 32},\n",
        "    {'hidden_layers': 3, 'hidden_size': 64},\n",
        "    {'hidden_layers': 4, 'hidden_size': 16},\n",
        "    {'hidden_layers': 4, 'hidden_size': 32},\n",
        "    {'hidden_layers': 4, 'hidden_size': 64}\n",
        "]\n",
        "\n",
        "training_methods = [\"full\", \"mini-batch\"]\n",
        "activation_func_names = [\"ReLU\", \"Sigmoid\", \"Arctan\"]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Loop through architectures, training methods, and activation functions\n",
        "for method in training_methods:\n",
        "    for arch in architectures:\n",
        "        for act_name in activation_func_names:\n",
        "            activation_func = activation_functions[act_name]\n",
        "\n",
        "            # Initialize model\n",
        "            model = SimpleNN(input_size, arch['hidden_layers'],\n",
        "                             arch['hidden_size'], output_size)\n",
        "\n",
        "            # Train the model\n",
        "            mini_batches = (method == \"mini-batch\")\n",
        "            start_time = time.time()  # Track training time\n",
        "            best_val_accuracy, epochs, model_updates, accuracies = train(\n",
        "                model, train_data, val_data, learning_rate, activation_func,\n",
        "                mini_batches, stop_condition\n",
        "            )\n",
        "            end_time = time.time()\n",
        "\n",
        "            training_time = end_time - start_time  # Calculate training time\n",
        "\n",
        "            # Test accuracy\n",
        "            test_accuracy = compute_accuracy(model, test_data, activation_func)\n",
        "\n",
        "            # Record results\n",
        "            results.append({\n",
        "                \"Method\": method,\n",
        "                \"Activation\": act_name,\n",
        "                \"Hidden Layers\": arch[\"hidden_layers\"],\n",
        "                \"Hidden Size\": arch[\"hidden_size\"],\n",
        "                \"Best Validation Accuracy\": best_val_accuracy,\n",
        "                \"Test Accuracy\": test_accuracy,\n",
        "                \"Epochs\": epochs,\n",
        "                \"Training Time (s)\": training_time,\n",
        "                \"Model Updates\": model_updates\n",
        "            })\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Results Table:\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot validation accuracy over time for one model\n",
        "plt.plot(accuracies)\n",
        "plt.title(\"Validation Accuracy Over Time\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# Analyzing results\n",
        "best_result = results_df.loc[results_df[\"Best Validation Accuracy\"].idxmax()]\n",
        "print(\"Best Architecture:\")\n",
        "print(best_result)\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\nObservations:\")\n",
        "mini_batch_results = results_df[results_df[\"Method\"] == \"mini-batch\"]\n",
        "full_batch_results = results_df[results_df[\"Method\"] == \"full\"]\n",
        "\n",
        "print(\"Did mini-batches help? What observations can you make from the data?\")\n",
        "if mini_batch_results[\"Best Validation Accuracy\"].mean() > (\n",
        "    full_batch_results[\"Best Validation Accuracy\"].mean()):\n",
        "    print(\"Mini-batches generally yielded better results.\")\n",
        "else:\n",
        "    print(\"Full-batch training generally yielded better results.\")\n",
        "\n",
        "print(\"Additional Observations:\")\n",
        "if best_result[\"Method\"] == \"mini-batch\":\n",
        "    print(\"The best performing architecture used mini-batch training.\")\n",
        "else:\n",
        "    print(\"The best performing architecture used full-batch training.\")"
      ],
      "metadata": {
        "id": "t-9s5ehxXfE7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14d4f485-519f-4c9c-c22f-842b91e0db37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-09a9ab84e829>:18: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Table:\n",
            "        Method Activation  Hidden Layers  Hidden Size  \\\n",
            "0         full       ReLU              2           16   \n",
            "1         full    Sigmoid              2           16   \n",
            "2         full     Arctan              2           16   \n",
            "3         full       ReLU              2           32   \n",
            "4         full    Sigmoid              2           32   \n",
            "5         full     Arctan              2           32   \n",
            "6         full       ReLU              2           64   \n",
            "7         full    Sigmoid              2           64   \n",
            "8         full     Arctan              2           64   \n",
            "9         full       ReLU              3           16   \n",
            "10        full    Sigmoid              3           16   \n",
            "11        full     Arctan              3           16   \n",
            "12        full       ReLU              3           32   \n",
            "13        full    Sigmoid              3           32   \n",
            "14        full     Arctan              3           32   \n",
            "15        full       ReLU              3           64   \n",
            "16        full    Sigmoid              3           64   \n",
            "17        full     Arctan              3           64   \n",
            "18        full       ReLU              4           16   \n",
            "19        full    Sigmoid              4           16   \n",
            "20        full     Arctan              4           16   \n",
            "21        full       ReLU              4           32   \n",
            "22        full    Sigmoid              4           32   \n",
            "23        full     Arctan              4           32   \n",
            "24        full       ReLU              4           64   \n",
            "25        full    Sigmoid              4           64   \n",
            "26        full     Arctan              4           64   \n",
            "27  mini-batch       ReLU              2           16   \n",
            "28  mini-batch    Sigmoid              2           16   \n",
            "29  mini-batch     Arctan              2           16   \n",
            "30  mini-batch       ReLU              2           32   \n",
            "31  mini-batch    Sigmoid              2           32   \n",
            "32  mini-batch     Arctan              2           32   \n",
            "33  mini-batch       ReLU              2           64   \n",
            "34  mini-batch    Sigmoid              2           64   \n",
            "35  mini-batch     Arctan              2           64   \n",
            "36  mini-batch       ReLU              3           16   \n",
            "37  mini-batch    Sigmoid              3           16   \n",
            "38  mini-batch     Arctan              3           16   \n",
            "39  mini-batch       ReLU              3           32   \n",
            "40  mini-batch    Sigmoid              3           32   \n",
            "41  mini-batch     Arctan              3           32   \n",
            "42  mini-batch       ReLU              3           64   \n",
            "43  mini-batch    Sigmoid              3           64   \n",
            "44  mini-batch     Arctan              3           64   \n",
            "45  mini-batch       ReLU              4           16   \n",
            "46  mini-batch    Sigmoid              4           16   \n",
            "47  mini-batch     Arctan              4           16   \n",
            "48  mini-batch       ReLU              4           32   \n",
            "49  mini-batch    Sigmoid              4           32   \n",
            "50  mini-batch     Arctan              4           32   \n",
            "51  mini-batch       ReLU              4           64   \n",
            "52  mini-batch    Sigmoid              4           64   \n",
            "53  mini-batch     Arctan              4           64   \n",
            "\n",
            "    Best Validation Accuracy  Test Accuracy  Epochs  Training Time (s)  \\\n",
            "0                       0.10           0.07       6           0.596294   \n",
            "1                       0.14           0.10      12           1.692551   \n",
            "2                       0.20           0.12       8           0.867079   \n",
            "3                       0.10           0.10       6           0.636362   \n",
            "4                       0.17           0.17       6           0.883341   \n",
            "5                       0.26           0.23      11           1.162433   \n",
            "6                       0.10           0.10       6           0.699489   \n",
            "7                       0.30           0.13      17           3.143630   \n",
            "8                       0.17           0.10      12           2.081404   \n",
            "9                       0.10           0.10       6           0.694882   \n",
            "10                      0.10           0.10       6           1.017170   \n",
            "11                      0.23           0.15      15           1.790756   \n",
            "12                      0.10           0.10       6           0.658517   \n",
            "13                      0.18           0.10      10           1.640646   \n",
            "14                      0.14           0.10       6           0.731200   \n",
            "15                      0.10           0.10       6           0.896188   \n",
            "16                      0.17           0.11       8           1.585072   \n",
            "17                      0.15           0.14       9           1.659864   \n",
            "18                      0.10           0.05       6           1.259249   \n",
            "19                      0.11           0.10      11           2.101444   \n",
            "20                      0.14           0.04       6           0.809740   \n",
            "21                      0.10           0.10       6           0.805300   \n",
            "22                      0.10           0.10       6           1.231673   \n",
            "23                      0.14           0.15      13           1.822371   \n",
            "24                      0.10           0.10       6           0.997458   \n",
            "25                      0.22           0.15      19           5.371025   \n",
            "26                      0.17           0.09      11           1.878403   \n",
            "27                      0.11           0.10       7           0.594213   \n",
            "28                      0.10           0.10       6           0.757952   \n",
            "29                      0.40           0.28      15           1.294533   \n",
            "30                      0.09           0.10       8           0.723563   \n",
            "31                      0.57           0.31      18           2.262615   \n",
            "32                      0.67           0.51      38           4.933554   \n",
            "33                      0.10           0.10       6           1.167514   \n",
            "34                      0.52           0.45      19           2.975844   \n",
            "35                      0.65           0.47      29           3.183702   \n",
            "36                      0.10           0.10       6           0.624892   \n",
            "37                      0.10           0.10       6           0.904867   \n",
            "38                      0.47           0.39      19           2.244402   \n",
            "39                      0.10           0.10       6           0.994247   \n",
            "40                      0.29           0.26      14           2.725720   \n",
            "41                      0.49           0.34      20           2.348439   \n",
            "42                      0.10           0.10       6           0.819884   \n",
            "43                      0.31           0.27       9           1.734385   \n",
            "44                      0.39           0.27      12           1.797788   \n",
            "45                      0.10           0.10       6           0.760725   \n",
            "46                      0.10           0.10       6           1.343866   \n",
            "47                      0.23           0.14      10           2.067441   \n",
            "48                      0.10           0.10       6           0.806368   \n",
            "49                      0.10           0.11       6           1.204083   \n",
            "50                      0.21           0.10       9           1.324616   \n",
            "51                      0.10           0.10       6           1.035608   \n",
            "52                      0.21           0.12      10           2.379955   \n",
            "53                      0.37           0.23      19           3.463737   \n",
            "\n",
            "    Model Updates  \n",
            "0               6  \n",
            "1              12  \n",
            "2               8  \n",
            "3               6  \n",
            "4               6  \n",
            "5              11  \n",
            "6               6  \n",
            "7              17  \n",
            "8              12  \n",
            "9               6  \n",
            "10              6  \n",
            "11             15  \n",
            "12              6  \n",
            "13             10  \n",
            "14              6  \n",
            "15              6  \n",
            "16              8  \n",
            "17              9  \n",
            "18              6  \n",
            "19             11  \n",
            "20              6  \n",
            "21              6  \n",
            "22              6  \n",
            "23             13  \n",
            "24              6  \n",
            "25             19  \n",
            "26             11  \n",
            "27             56  \n",
            "28             48  \n",
            "29            120  \n",
            "30             64  \n",
            "31            144  \n",
            "32            304  \n",
            "33             48  \n",
            "34            152  \n",
            "35            232  \n",
            "36             48  \n",
            "37             48  \n",
            "38            152  \n",
            "39             48  \n",
            "40            112  \n",
            "41            160  \n",
            "42             48  \n",
            "43             72  \n",
            "44             96  \n",
            "45             48  \n",
            "46             48  \n",
            "47             80  \n",
            "48             48  \n",
            "49             48  \n",
            "50             72  \n",
            "51             48  \n",
            "52             80  \n",
            "53            152  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Architecture:\n",
            "Method                      mini-batch\n",
            "Activation                      Arctan\n",
            "Hidden Layers                        2\n",
            "Hidden Size                         32\n",
            "Best Validation Accuracy          0.67\n",
            "Test Accuracy                     0.51\n",
            "Epochs                              38\n",
            "Training Time (s)             4.933554\n",
            "Model Updates                      304\n",
            "Name: 32, dtype: object\n",
            "\n",
            "Observations:\n",
            "Did mini-batches help? What observations can you make from the data?\n",
            "Mini-batches generally yielded better results.\n",
            "Additional Observations:\n",
            "The best performing architecture used mini-batch training.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}